{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lakefs-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNc3EhNgSilqv8DQwf6/Kf7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrea-rockt/Jarvis/blob/master/lakefs_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lakefs lab\n",
        "\n",
        "We need to build our own enviroment to test things out and learn about git like file systems\n",
        "\n",
        "* as data engineers we want to try things out in order to properly understand systems that we are building.\n",
        "\n",
        "* as data scientist we want an environment able to support our experimentations.\n",
        "\n",
        "* as developers we want reproducible enviroments to validate our code on.\n",
        "\n",
        "Let's begin by configuring our environment, nobody has ever been fired by defining a bit of infrastructure.\n",
        "\n",
        "We are going to create an environment based on\n",
        "\n",
        "* Apache Spark: our distributed execution engine, this will be the compute layer of our lab environment and will shuffle data around your cluster and crunch the numbers.\n",
        "* The local filesystem: we need to store the actual data on a distributed filesystem, we are only going to only use one node so we will select the local filesystem viewing it as a *special* case of a more general distributed filesystem.\n",
        "* Lakefs: our metadata management solution, table formats describe plain files as collection of related content by attaching metadata to those files, we will store this metadata inside lakefs in order to get time travel on metadata.  \n",
        "\n",
        "# Installing prerequisites\n",
        "\n",
        "We are going to configure this colab instance by:\n",
        "\n",
        "* downloading `spark-3.1.2`\n",
        "* postgresql 11\n",
        "* downloading a binary distribution of `lakefs`\n",
        "* downloading `ngrok`\n",
        "\n",
        "We are going to access web uis via tunnels provided by `ngrok` (register with your github account or google account on `ngrok.com` and get your auth token)\n",
        "\n",
        "replace `THE_AUTH_TOKEN_FOR_NGROK` with your actual auth token"
      ],
      "metadata": {
        "id": "aBvUMnXpR11w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCZhFXziViXn"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "echo \"Installing SPARK\"\n",
        "wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "echo \"Installing FINDSPARK\"\n",
        "pip -q install findspark "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "echo \"Installing POSTGRESQL 11\"\n",
        "wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n",
        "RELEASE=$(lsb_release -cs)\n",
        "echo \"deb http://apt.postgresql.org/pub/repos/apt/ ${RELEASE}\"-pgdg main | sudo tee  /etc/apt/sources.list.d/pgdg.list\n",
        "sudo apt update -qq > /dev/null\n",
        "sudo apt -y -qq install postgresql-11 > /dev/null\n",
        "sudo service postgresql start\n",
        "sudo -u postgres -- psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"\n",
        "sudo -u postgres -- psql -U postgres -c \"CREATE DATABASE lakefs;\""
      ],
      "metadata": {
        "id": "SYrVOLnpzJDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "mkdir -p /lakefs\n",
        "wget -q https://github.com/treeverse/lakeFS/releases/download/v0.57.2/lakeFS_0.57.2_Linux_x86_64.tar.gz\n",
        "tar xf lakeFS_0.57.2_Linux_x86_64.tar.gz\n",
        "export LAKEFS_LOGGING_OUTPUT='-'\n",
        "export LAKEFS_BLOCKSTORE_LOCAL_PATH='/lakefs'\n",
        "export LAKEFS_DATABASE_CONNECTION_STRING='postgres://postgres:postgres@localhost:5432/lakefs?sslmode=disable'\n",
        "export LAKEFS_LOGGING_FORMAT='text'\n",
        "export LAKEFS_BLOCKSTORE_TYPE='local'\n",
        "export LAKEFS_GATEWAYS_S3_REGION='us-east-1'\n",
        "export LAKEFS_AUTH_ENCRYPT_SECRET_KEY='10a718b3f285d89c36e9864494cdd1507f3bc85b342df24736ea81f9a1134bcc09e90b6641'\n",
        "export LAKEFS_LOGGING_LEVEL='DEBUG'\n",
        "export LAKEFS_LISTEN_ADDRESS='0.0.0.0:8000'\n",
        "nohup ./lakefs run > lakefs.log &"
      ],
      "metadata": {
        "id": "WyRnd0cpmked"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
        "tar xf ngrok-stable-linux-amd64.tgz\n",
        "./ngrok authtoken 5spccTgusz1o9Uve5mvJU_5oB3MMchu5ct6FbifpLLz\n",
        "nohup ./ngrok http 8000 > ngrok.log &"
      ],
      "metadata": {
        "id": "ilWtvV_9n_99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env LAKECTL_CREDENTIALS_ACCESS_KEY_ID=AKIAJUQLSCYMGWXURKDQ\n",
        "%env LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY=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\n",
        "%env LAKECTL_SERVER_ENDPOINT_URL=http://localhost:8000"
      ],
      "metadata": {
        "id": "-Ur8yZXJLjFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./lakectl branch create --source lakefs://colab/main lakefs://colab/dev"
      ],
      "metadata": {
        "id": "9guvUlqp1XpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "CREDENTIALS_ACCESS_KEY_ID=os.environ[\"LAKECTL_CREDENTIALS_ACCESS_KEY_ID\"]\n",
        "CREDENTIALS_SECRET_ACCESS_KEY=os.environ[\"LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY\"]\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"spark-lakefs-training\") \\\n",
        "       .config(\"spark.jars.packages\",\n",
        "              \"org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
        "       .config(\"spark.sql.extensions\", \n",
        "              \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
        "       .config('spark.hadoop.fs.s3a.access.key',CREDENTIALS_ACCESS_KEY_ID) \\\n",
        "       .config('spark.hadoop.fs.s3a.secret.key',CREDENTIALS_SECRET_ACCESS_KEY) \\\n",
        "       .config('spark.hadoop.fs.s3a.path.style.access',True) \\\n",
        "       .config('spark.hadoop.fs.s3a.endpoint','http://localhost:8000') \\\n",
        "       .getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "xbrcpfNr1zkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/sivabalanb/Data-Analysis-with-Pandas-and-Python/raw/master/nba.csv"
      ],
      "metadata": {
        "id": "LgFhPWaK8FVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, DecimalType\n",
        "from pyspark.sql.functions import mean\n",
        "playersSchema = StructType([\n",
        "  StructField(\"Name\",StringType(),False), \\\n",
        "  StructField(\"Team\",StringType(),True), \\\n",
        "  StructField(\"Number\",StringType(),True), \\\n",
        "  StructField(\"Position\", StringType(), True), \\\n",
        "  StructField(\"Age\", StringType(), True), \\\n",
        "  StructField(\"Height\", StringType(), True), \\\n",
        "  StructField(\"Weight\", DoubleType(), True), \\\n",
        "  StructField(\"College\", StringType(), True), \\\n",
        "  StructField(\"Salary\", DecimalType(14, 2), True)\n",
        "])\n"
      ],
      "metadata": {
        "id": "uO1LXEHz8K6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "playersDfRaw = spark.read.csv('nba.csv', header=True, schema=playersSchema)\n",
        "playersDf = playersDfRaw.select(playersDfRaw.Name,\n",
        "                                playersDfRaw.Team,\n",
        "                                playersDfRaw.Number.cast(IntegerType()),\n",
        "                                playersDfRaw.Position,\n",
        "                                playersDfRaw.Age.cast(IntegerType()),\n",
        "                                playersDfRaw.Height,\n",
        "                                playersDfRaw.Weight,\n",
        "                                playersDfRaw.College,\n",
        "                                playersDfRaw.Salary)\n",
        "\n",
        "playersDf.write.parquet('s3a://colab/dev/nba/player')\n",
        "playersDf.groupBy('Position').agg(mean('Salary').alias('MeanSalary')).write.parquet('s3a://colab/dev/nba/salary')"
      ],
      "metadata": {
        "id": "wKEFkeiB8MjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./lakectl commit lakefs://colab/dev -m \"Initial load of nba tables\""
      ],
      "metadata": {
        "id": "ARJu3Lv58mlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./lakectl merge  lakefs://colab/dev lakefs://colab/main "
      ],
      "metadata": {
        "id": "AIsdQ3A4DJBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.parquet('s3a://colab/dev/nba/player').createOrReplaceTempView('player')\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT \n",
        "  SUM(CAST (Salary   is NULL as INTEGER)) as null_salaries,\n",
        "  SUM(CAST (College  is NULL as INTEGER)) as null_college,\n",
        "  SUM(CAST (Weight   is NULL as INTEGER)) as null_weight,\n",
        "  SUM(CAST (Height   is NULL as INTEGER)) as null_height,\n",
        "  SUM(CAST (Age      is NULL as INTEGER)) as null_age,\n",
        "  SUM(CAST (Position is NULL as INTEGER)) as null_position,\n",
        "  SUM(CAST (Number   is NULL as INTEGER)) as null_number,\n",
        "  SUM(CAST (Team     is NULL as INTEGER)) as null_team,\n",
        "  SUM(CAST (Name     is NULL as INTEGER)) as null_name\n",
        "FROM \n",
        "  player \n",
        "\"\"\").toPandas()"
      ],
      "metadata": {
        "id": "_lw_-CqaD4YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./lakectl fs rm --recursive lakefs://colab/dev/nba"
      ],
      "metadata": {
        "id": "D8wfC5abD_tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "playersDfRaw = spark.read.csv('nba.csv', header=True, schema=playersSchema)\n",
        "playersDf = playersDfRaw.select(playersDfRaw.Name,\n",
        "                                playersDfRaw.Team,\n",
        "                                playersDfRaw.Number.cast(IntegerType()),\n",
        "                                playersDfRaw.Position,\n",
        "                                playersDfRaw.Age.cast(IntegerType()),\n",
        "                                playersDfRaw.Height,\n",
        "                                playersDfRaw.Weight,\n",
        "                                playersDfRaw.College,\n",
        "                                playersDfRaw.Salary).createOrReplaceTempView('player')\n",
        "\n",
        "\n",
        "updatedDF = spark.sql(\n",
        "\"\"\"\n",
        "SELECT *\n",
        "FROM player\n",
        "WHERE\n",
        "NOT(\n",
        "Salary is NULL AND\n",
        "College is NULL AND\n",
        "Weight is NULL AND\n",
        "Height is NULL AND\n",
        "Age is NULL AND\n",
        "Position is NULL AND\n",
        "Number is NULL AND\n",
        "Team is NULL AND\n",
        "Name is NULL\n",
        ") \n",
        "\"\"\")\n",
        "\n",
        "updatedDF.write.parquet('s3a://colab/dev/nba/player')\n",
        "updatedDF.groupBy('Position').agg(mean('Salary').alias('MeanSalary')).write.parquet('s3a://colab/dev/nba/salary')\n"
      ],
      "metadata": {
        "id": "KxKrwQexE3Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.parquet('s3a://colab/main/nba/player').where('Weight is  Null').toPandas()"
      ],
      "metadata": {
        "id": "IVWetHXNFSak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./lakectl commit lakefs://colab/dev -m \"Removed null rows\"\n"
      ],
      "metadata": {
        "id": "wycKuCr1GGPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./lakectl merge lakefs://colab/dev lakefs://colab/main "
      ],
      "metadata": {
        "id": "-F2uC9KOQ7x5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}